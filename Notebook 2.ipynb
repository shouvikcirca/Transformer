{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import fasttext as ft\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder():\n",
    "    def __init__(self, vectorRepresentations):\n",
    "        self.vectorRepresentations = vectorRepresentations\n",
    "        self.d_model = 10\n",
    "        self.new_dim = 5\n",
    "\n",
    "\n",
    "    def PositionalEncoding(self,wordVecs):\n",
    "        for pos in range(wordVecs.shape[0]):\n",
    "            for i in range(wordVecs[pos].shape[0]):\n",
    "                if i%2 == 0:\n",
    "                    wordVecs[pos][i] = wordVecs[pos][i] + math.sin(pos/(10000**(2*i/self.d_model)))\n",
    "                else:\n",
    "                    wordVecs[pos][i] = wordVecs[pos][i] + math.cos(pos/(10000**(2*i/self.d_model)))            \n",
    "        return wordVecs\n",
    "\n",
    "\n",
    "    def get_qkv_weights(self,r,c):\n",
    "        query_weights = torch.rand((r,c))\n",
    "        key_weights = torch.rand((r,c))\n",
    "        value_weights = torch.rand((r,c))\n",
    "        return query_weights, key_weights, value_weights\n",
    "    \n",
    "    \n",
    "    \n",
    "    def qkvs(self,vectorMatrix, new_dim):\n",
    "        query_weights, key_weights, value_weights = self.get_qkv_weights(self.d_model,new_dim)\n",
    "        return torch.matmul(vectorMatrix, query_weights), torch.matmul(vectorMatrix, key_weights), \\\n",
    "        torch.matmul(vectorMatrix, value_weights) \n",
    "        # Check for transposeness in matrix multiplication\n",
    "    \n",
    "    \n",
    "    def qk_dotproducts(self,queries, keys):\n",
    "        dotproduct_matrix = torch.Tensor([])\n",
    "        for i in queries:\n",
    "            dotproduct_vector = torch.Tensor([])\n",
    "            for j in keys:\n",
    "                dotproduct_vector = torch.cat([dotproduct_vector, torch.dot(i,j).reshape(-1)])\n",
    "            dotproduct_matrix = torch.cat([dotproduct_matrix, dotproduct_vector.reshape(1,-1)])\n",
    "        return dotproduct_matrix\n",
    "    \n",
    "    \n",
    "    def getSoftmaxed_qkdp(self,qk_dotproductmatrix):\n",
    "        sm = nn.Softmax(dim = 0)\n",
    "        sm_matrix = torch.tensor([])\n",
    "        for i in qk_dotproductmatrix:\n",
    "            sm_matrix = torch.cat([sm_matrix, sm(i).reshape(1,-1)])\n",
    "        return sm_matrix\n",
    "    \n",
    "    \n",
    "    def getSoftmaxWeightedValues(self,softmaxed_qkdp, values):\n",
    "        dim2_mat = torch.tensor([])\n",
    "        dim3_mat = torch.tensor([])\n",
    "        outer_loop_range = softmaxed_qkdp.shape[0]\n",
    "        inner_loop_range = values.shape[0]\n",
    "        for i in range(outer_loop_range):\n",
    "            for j in range(inner_loop_range):\n",
    "                dim2_mat = torch.cat([dim2_mat, (softmaxed_qkdp[i][j]*values[j]).reshape(-1)])\n",
    "            dim3_mat = torch.cat([dim3_mat, dim2_mat.reshape(1,values.shape[0],values.shape[1])])\n",
    "            dim2_mat = torch.tensor([]) \n",
    "        return dim3_mat\n",
    "    \n",
    "    \n",
    "    def returnRepresentation(self):\n",
    "        pos_encoded = self.PositionalEncoding(self.vectorRepresentations)\n",
    "        new_dim = self.new_dim\n",
    "        queries, keys, values = self.qkvs(pos_encoded, new_dim)\n",
    "        qk_dotproductmatrix = self.qk_dotproducts(queries, keys)\n",
    "        d_k = keys.shape[1] # to be changed later to square root of 'key' vector dimension\n",
    "        qk_dotproductmatrix/=d_k\n",
    "        softmaxed_qkdp = self.getSoftmaxed_qkdp(qk_dotproductmatrix)\n",
    "        softmax_weighted_values = self.getSoftmaxWeightedValues(softmaxed_qkdp, values)\n",
    "        return softmax_weighted_values\n",
    "                                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordVectors(sentence):\n",
    "    sentence = sentence.split(' ')\n",
    "    vecs = torch.rand((len(sentence),10))\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVecs = getWordVectors('Hi there this is nuts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[7.4512e-01, 1.0554e+00, 7.8300e-01, 7.4537e-01, 9.6133e-01],\n",
       "         [1.2013e+00, 1.5676e+00, 1.1543e+00, 1.2686e+00, 1.5323e+00],\n",
       "         [3.4198e+00, 3.9404e+00, 2.6120e+00, 3.4701e+00, 4.1074e+00],\n",
       "         [1.3480e-04, 1.7156e-04, 1.1612e-04, 1.3747e-04, 1.6242e-04],\n",
       "         [4.3894e-04, 6.7405e-04, 5.1444e-04, 4.3864e-04, 5.5125e-04]],\n",
       "\n",
       "        [[7.3713e-01, 1.0440e+00, 7.7460e-01, 7.3737e-01, 9.5102e-01],\n",
       "         [1.2079e+00, 1.5762e+00, 1.1607e+00, 1.2756e+00, 1.5407e+00],\n",
       "         [3.4220e+00, 3.9429e+00, 2.6137e+00, 3.4723e+00, 4.1100e+00],\n",
       "         [1.0125e-04, 1.2887e-04, 8.7223e-05, 1.0326e-04, 1.2199e-04],\n",
       "         [3.6273e-04, 5.5702e-04, 4.2512e-04, 3.6248e-04, 4.5554e-04]],\n",
       "\n",
       "        [[7.6089e-01, 1.0777e+00, 7.9957e-01, 7.6114e-01, 9.8168e-01],\n",
       "         [1.2070e+00, 1.5751e+00, 1.1598e+00, 1.2747e+00, 1.5396e+00],\n",
       "         [3.3949e+00, 3.9117e+00, 2.5930e+00, 3.4448e+00, 4.0775e+00],\n",
       "         [1.2326e-04, 1.5688e-04, 1.0618e-04, 1.2571e-04, 1.4852e-04],\n",
       "         [4.2101e-04, 6.4653e-04, 4.9343e-04, 4.2073e-04, 5.2874e-04]],\n",
       "\n",
       "        [[9.9032e-01, 1.4027e+00, 1.0407e+00, 9.9065e-01, 1.2777e+00],\n",
       "         [1.3915e+00, 1.8158e+00, 1.3370e+00, 1.4694e+00, 1.7749e+00],\n",
       "         [2.9054e+00, 3.3477e+00, 2.2191e+00, 2.9481e+00, 3.4895e+00],\n",
       "         [2.3696e-03, 3.0159e-03, 2.0413e-03, 2.4166e-03, 2.8551e-03],\n",
       "         [5.5884e-03, 8.5818e-03, 6.5496e-03, 5.5846e-03, 7.0183e-03]],\n",
       "\n",
       "        [[9.2667e-01, 1.3125e+00, 9.7377e-01, 9.2698e-01, 1.1956e+00],\n",
       "         [1.3674e+00, 1.7844e+00, 1.3139e+00, 1.4440e+00, 1.7442e+00],\n",
       "         [3.0131e+00, 3.4718e+00, 2.3014e+00, 3.0574e+00, 3.6190e+00],\n",
       "         [1.1566e-03, 1.4721e-03, 9.9638e-04, 1.1796e-03, 1.3936e-03],\n",
       "         [3.1116e-03, 4.7783e-03, 3.6468e-03, 3.1095e-03, 3.9078e-03]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Encoder(wordVecs)\n",
    "a.returnRepresentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
